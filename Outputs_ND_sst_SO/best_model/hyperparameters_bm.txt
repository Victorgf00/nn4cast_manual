{'num_layers': 5, 'units_0': 256, 'units_1': 256, 'activations_0': 'relu', 'kernel_regularizer': 'l2', 'batch_normalization': 'True', 'he_initialization': 'True', 'dropout': 'False', 'learning_rate': 0.001, 'dropout_0': 0.0, 'units_2': 64, 'units_3': 64, 'activations_1': 'relu', 'activations_2': 'elu', 'dropout_1': 0.0, 'dropout_2': 0.0, 'units_4': 16, 'activations_3': 'relu', 'dropout_3': 0.0}